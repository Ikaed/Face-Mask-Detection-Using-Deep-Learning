{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# Project 1: Face Mask Detector\n",
        "# \n",
        "#         Group 2\n",
        "# \n",
        "#       Mansur Rahman\n",
        "#   Mikael Falk Lundgren\n",
        "#       Yann Reibel\n",
        "#################################"
      ],
      "metadata": {
        "id": "YxO_Ym0J2vp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBoguQtFqVPY"
      },
      "outputs": [],
      "source": [
        "# This sets up, creates and trains model 1 \n",
        "import os\n",
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"./data/\")\n",
        "image_path = data_path / \"Face_Mask_Dataset\"\n",
        "# Setup train, testing and validation paths\n",
        "train_dir = image_path / \"Train\"\n",
        "test_dir = image_path / \"Test\"\n",
        "val_dir = image_path / \"Validation\"\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "# Traverses and prints directory details\n",
        "walk_through_dir(image_path)\n",
        "\n",
        "# Transform (with 1 data aug to improve performance) to tensor\n",
        "data_transform = transforms.Compose([transforms.Resize(80), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor()])\n",
        "\n",
        "# Create pytorch datasets for train, test and validation\n",
        "train_data = datasets.ImageFolder(root=train_dir, transform=data_transform, target_transform=None)\n",
        "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
        "\n",
        "# Checking the data and a sample\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n",
        "img_1, label_1 = train_data[0][0], train_data[0][1]\n",
        "print(f\"Image tensor:\\n{img_1}\")\n",
        "print(f\"Image shape: {img_1.shape}\")\n",
        "print(f\"Image datatype: {img_1.dtype}\")\n",
        "print(f\"Image label: {label_1}\")\n",
        "print(f\"Label datatype: {type(label_1)}\")\n",
        "\n",
        "# Checks for class names\n",
        "class_names = train_data.classes\n",
        "print(f\"Class Names = {class_names}\")\n",
        "\n",
        "# Turn train and test Datasets into DataLoaders\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=16, num_workers=1, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=16, num_workers=1, shuffle=False)\n",
        "\n",
        "# Check if the Dataloaders look fine\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of 16\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of 16\")\n",
        "\n",
        "# Print to check shape is fine\n",
        "img_2, label_2 = next(iter(train_dataloader))\n",
        "print(f\"Image shape: {img_2.shape} -> [batch_size, color_channels, height, width]\")\n",
        "\n",
        "# Create a convolutional neural network \n",
        "class FaceMask(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=3, \n",
        "                out_channels=6, \n",
        "                kernel_size=3, \n",
        "                stride=1, \n",
        "                padding=1),\n",
        "\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=6, \n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2,\n",
        "      stride=2) \n",
        ")\n",
        "\n",
        "    self.block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=16, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=20, out_channels=24, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2)\n",
        ")\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(6*40*40,output_shape)\n",
        ")\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.block_1(x)\n",
        "    x = self.block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "# Create first model\n",
        "model_0 = FaceMask(input_shape=3, hidden_units=10, output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, \n",
        "optimizer: torch.optim.Optimizer):\n",
        "# Put model in train mode\n",
        "  model.train()\n",
        "# Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "# Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "# Send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "# 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "# 2. Calculate and accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item() \n",
        "# 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "# 4. Loss backward\n",
        "    loss.backward()\n",
        "# 5. Optimizer step\n",
        "    optimizer.step()\n",
        "# Calculate and accumulate accuracy metric across all batches\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "# Adjust metrics to get average loss and accuracy per batch \n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module, \n",
        "    dataloader: torch.utils.data.DataLoader, \n",
        "    loss_fn: torch.nn.Module):\n",
        "# Put model in eval mode\n",
        "  model.eval() \n",
        "# Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "# Turn on no_grad context manager\n",
        "  with torch.no_grad():\n",
        "# Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "# Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "# 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "# 2. Calculate and accumulate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "# Calculate and accumulate accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "# Adjust metrics to get average loss and accuracy per batch \n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "# Setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "# Combine train_step & test_step functions\n",
        "# 1. Take in various parameters required for training and test steps\n",
        "def train(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, \n",
        "          optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module = nn.CrossEntropyLoss(), epochs: int = 5):\n",
        "    # 2. Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "    # 3. Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,dataloader=train_dataloader, loss_fn=loss_fn, optimizer=optimizer)\n",
        "        test_loss, test_acc = test_step(model=model, dataloader=test_dataloader, loss_fn=loss_fn)        \n",
        "        # 4. Print loss & accuracies\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "        # 5. Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "    # 6. Return the filled results at the end of the epochs\n",
        "    return results\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "model_0_results = train(model=model_0, \n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn, \n",
        "                        epochs=NUM_EPOCHS)\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n",
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    \n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot \n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(model_0_results)\n",
        "\n",
        "# Save model_0\n",
        "# Create models directory (if it doesn't already exist)\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # Create parent directories if needed\n",
        "exist_ok=True # Allows exisiting directory\n",
        ")\n",
        "# Create model save path\n",
        "MODEL_NAME = \"model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(), \n",
        "f=MODEL_SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This sets up, creates and trains model 2\n",
        "import os\n",
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"./data/\")\n",
        "image_path = data_path / \"Face_Mask_Dataset\"\n",
        "# Setup train, testing and validation paths\n",
        "train_dir = image_path / \"Train\"\n",
        "test_dir = image_path / \"Test\"\n",
        "val_dir = image_path / \"Validation\"\n",
        "\n",
        "# Traverse through the data directory and list contents\n",
        "def walk_through_dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "# Traverses and prints directory details\n",
        "walk_through_dir(image_path)\n",
        "\n",
        "# Transform (with 1 data aug to improve performance) to tensor\n",
        "data_transform = transforms.Compose([transforms.Resize(64), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor()])\n",
        "\n",
        "# Create pytorch datasets for train, test and validation\n",
        "train_data = datasets.ImageFolder(root=train_dir, transform=data_transform, target_transform=None)\n",
        "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
        "\n",
        "# Checking the data and a sample\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n",
        "img_1, label_1 = train_data[0][0], train_data[0][1]\n",
        "print(f\"Image tensor:\\n{img_1}\")\n",
        "print(f\"Image shape: {img_1.shape}\")\n",
        "print(f\"Image datatype: {img_1.dtype}\")\n",
        "print(f\"Image label: {label_1}\")\n",
        "print(f\"Label datatype: {type(label_1)}\")\n",
        "\n",
        "# Checks for class names\n",
        "class_names = train_data.classes\n",
        "print(f\"Class Names = {class_names}\")\n",
        "\n",
        "# Turn train and test Datasets into DataLoaders\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=16, num_workers=1, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=16, num_workers=1, shuffle=False)\n",
        "\n",
        "# Check if the Dataloaders look fine\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of 16\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of 16\")\n",
        "\n",
        "# Print to check shape is fine\n",
        "img_2, label_2 = next(iter(train_dataloader))\n",
        "print(f\"Image shape: {img_2.shape} -> [batch_size, color_channels, height, width]\")\n",
        "\n",
        "# Create a convolutional neural network \n",
        "class FaceMask(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=3, \n",
        "                out_channels=6, \n",
        "                kernel_size=3, \n",
        "                stride=1, \n",
        "                padding=1),\n",
        "\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=6, \n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2,\n",
        "      stride=2) \n",
        ")\n",
        "\n",
        "    self.block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=16, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=20, out_channels=24, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2)\n",
        ")\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(6*32*32,output_shape)\n",
        ")\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.block_1(x)\n",
        "    x = self.block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "# Create first model\n",
        "model_0 = FaceMask(input_shape=3, hidden_units=10, output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, \n",
        "optimizer: torch.optim.Optimizer):\n",
        "# Put model in train mode\n",
        "  model.train()\n",
        "# Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "# Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "# Send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "# 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "# 2. Calculate and accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item() \n",
        "# 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "# 4. Loss backward\n",
        "    loss.backward()\n",
        "# 5. Optimizer step\n",
        "    optimizer.step()\n",
        "# Calculate and accumulate accuracy metric across all batches\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "# Adjust metrics to get average loss and accuracy per batch \n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module, \n",
        "    dataloader: torch.utils.data.DataLoader, \n",
        "    loss_fn: torch.nn.Module):\n",
        "# Put model in eval mode\n",
        "  model.eval() \n",
        "# Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "# Turn on no_grad context manager\n",
        "  with torch.no_grad():\n",
        "# Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "# Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "# 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "# 2. Calculate and accumulate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "# Calculate and accumulate accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "# Adjust metrics to get average loss and accuracy per batch \n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "# Setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "# Combine train_step & test_step functions\n",
        "# 1. Take in various parameters required for training and test steps\n",
        "def train(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, \n",
        "          optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module = nn.CrossEntropyLoss(), epochs: int = 5):\n",
        "    # 2. Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "    # 3. Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,dataloader=train_dataloader, loss_fn=loss_fn, optimizer=optimizer)\n",
        "        test_loss, test_acc = test_step(model=model, dataloader=test_dataloader, loss_fn=loss_fn)        \n",
        "        # 4. Print out what's happening\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "        # 5. Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "    # 6. Return the filled results at the end of the epochs\n",
        "    return results\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "model_0_results = train(model=model_0, \n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn, \n",
        "                        epochs=NUM_EPOCHS)\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n",
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    \n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot \n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(model_0_results)\n",
        "\n",
        "# Save model_0\n",
        "# Create models directory (if it doesn't already exist)\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # Create parent directories if needed\n",
        "exist_ok=True # Allows exisiting directory\n",
        ")\n",
        "# Create model save path\n",
        "MODEL_NAME = \"model_1.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_1.state_dict(), \n",
        "f=MODEL_SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "1L8YLiEnvQk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads a saved model and performs predictions on the validation set\n",
        "import os\n",
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create the same convolutional neural network that trained model_1\n",
        "class FaceMask(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape, \n",
        "                out_channels=hidden_units, \n",
        "                kernel_size=3, \n",
        "                stride=1, \n",
        "                padding=1),\n",
        "\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=6, \n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2,\n",
        "      stride=2) \n",
        ")\n",
        "\n",
        "    self.block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=16, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=20, out_channels=24, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2)\n",
        ")\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(6*32*32,output_shape)\n",
        ")\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.block_1(x)\n",
        "    x = self.block_2(x)    \n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "loaded_model_1 = FaceMask(input_shape=3, hidden_units=6, output_shape=2)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_NAME = \"model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "loaded_model_1.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "loaded_model_1 = loaded_model_1.to(device)\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"./data/\")\n",
        "image_path = data_path / \"Face_Mask_Dataset\"\n",
        "# Setup validation path\n",
        "val_dir = image_path / \"Validation\"\n",
        "\n",
        "# Transform (w/o data aug since validation) to tensor\n",
        "data_transform = transforms.Compose([transforms.Resize(64), \n",
        "transforms.ToTensor()])\n",
        "\n",
        "# Create pytorch datasets for validation\n",
        "val_data = datasets.ImageFolder(root=val_dir, transform=data_transform)\n",
        "\n",
        "# Turn Validation Datasets into DataLoader\n",
        "val_dataloader = DataLoader(dataset=val_data, batch_size=16, num_workers=1, shuffle=False)\n",
        "\n",
        "# Define function for predictions\n",
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = [] #Create an empty list for prediction probabilities \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sample in data:\n",
        "            # unsqueeze & add a batch size dimension on the sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device)\n",
        "            # Forward pass\n",
        "            pred_logit = model(sample) #pass the sample to our target model\n",
        "            # Get prediction probability from a given sample\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # since multi-class, use the softmax activation function on our pred logit\n",
        "            # MOve to cpu for matplotlib\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "    return torch.stack(pred_probs)\n",
        "\n",
        "# Validation Test - We validate our model against the validation set\n",
        "\n",
        "test_samples = [] #create empty list for validation data\n",
        "test_labels = []  # create empty list for corresponding labels\n",
        "for sample, label in list(val_data): # put correspinding data & label from validation set into above lists\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# Make predictions on test samples with model \n",
        "pred_probs= make_predictions(model=loaded_model_1, data=test_samples)\n",
        "\n",
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "\n",
        "\n",
        "count_correct = 0\n",
        "count_incorrect = 0\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Find the prediction label\n",
        "  pred_label = val_data.classes[pred_classes[i]]\n",
        "  # Get the truth label\n",
        "  truth_label = val_data.classes[test_labels[i]] \n",
        "  if pred_label == truth_label:\n",
        "      count_correct +=1\n",
        "  else:\n",
        "      count_incorrect +=1\n",
        "\n",
        "print('Number of correctly predicted images was:', count_correct,'\\nNumber of incorrectly predicted images was:', count_incorrect)\n",
        "print(f\"\\nAccuracy = {(count_correct)/8} %\") #There are a total of 800 images in validation set"
      ],
      "metadata": {
        "id": "7tRkyDx5p29G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads a saved model and predicts on a random sample of images from validation set\n",
        "# This loads the saved model and predicts on validation set\n",
        "import os\n",
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "import random\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create the same convolutional neural network that trained model_0\n",
        "class FaceMask(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape, \n",
        "                out_channels=hidden_units, \n",
        "                kernel_size=3, # how big is the square that's going over the image?\n",
        "                stride=1, # default\n",
        "                padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int \n",
        "\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=6, \n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2,\n",
        "      stride=2) # default stride value is same as kernel_size\n",
        ")\n",
        "\n",
        "    self.block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=16, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=20, out_channels=24, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2)\n",
        ")\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(6*32*32,output_shape)\n",
        ")\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.block_1(x)\n",
        "    x = self.block_2(x)    \n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "loaded_model_1 = FaceMask(input_shape=3, hidden_units=6, output_shape=2)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_NAME = \"model_2.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "loaded_model_1.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "loaded_model_1 = loaded_model_1.to(device)\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"./data/\")\n",
        "image_path = data_path / \"Face_Mask_Dataset\"\n",
        "# Setup validation path\n",
        "val_dir = image_path / \"Validation\"\n",
        "\n",
        "# Transform (w/o data aug since validation) to tensor\n",
        "data_transform = transforms.Compose([transforms.Resize(64), \n",
        "transforms.ToTensor()])\n",
        "\n",
        "# Create pytorch datasets for validation\n",
        "val_data = datasets.ImageFolder(root=val_dir, transform=data_transform)\n",
        "\n",
        "# Turn Validation Datasets into DataLoader\n",
        "val_dataloader = DataLoader(dataset=val_data, batch_size=16, num_workers=1, shuffle=False)\n",
        "\n",
        "# Define function for predictions\n",
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = [] #Create an empty list for prediction probabilities \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sample in data:\n",
        "            # unsqueeze & add a batch size dimension on the sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device)\n",
        "            # Forward pass\n",
        "            pred_logit = model(sample) #pass the sample to our target model\n",
        "            # Get prediction probability from a given sample\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # since multi-class, use the softmax activation function on our pred logit\n",
        "            # MOve to cpu for matplotlib\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "    return torch.stack(pred_probs)\n",
        "\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(val_data), k=25): # to randomly sample 25 images from validation set\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# Make predictions on test samples with model 1\n",
        "pred_probs= make_predictions(model=loaded_model_1, data=test_samples)\n",
        "\n",
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "\n",
        "# Plot predictions\n",
        "plt.figure(figsize=(25, 25))\n",
        "nrows = 9\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze().permute(1, 2, 0), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label\n",
        "  pred_label = val_data.classes[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label\n",
        "  truth_label = val_data.classes[test_labels[i]] \n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "  \n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\", pad=3) # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\", pad=3) # red text if wrong\n",
        "  plt.axis(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3RMcFxvWBHuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}